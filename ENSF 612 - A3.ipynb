{"cells":[{"cell_type":"markdown","source":["d\n## ENSF 612 - Assignment 3\n### Author: Steven Duong (30022492)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3333321b-0c37-4036-b465-86276a00653e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install nltk\n%pip install pyspellchecker\n%pip install beautifulsoup4"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9abe4591-2dec-48ca-ae72-fd1466e32640","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.3)\nCollecting regex>=2021.8.3\n  Downloading regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.0.1)\nInstalling collected packages: tqdm, regex, nltk\nSuccessfully installed nltk-3.8.1 regex-2022.10.31 tqdm-4.64.1\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting pyspellchecker\n  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\nInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.7.1\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting beautifulsoup4\n  Downloading beautifulsoup4-4.11.2-py3-none-any.whl (129 kB)\nCollecting soupsieve>1.2\n  Downloading soupsieve-2.4-py3-none-any.whl (37 kB)\nInstalling collected packages: soupsieve, beautifulsoup4\nSuccessfully installed beautifulsoup4-4.11.2 soupsieve-2.4\nPython interpreter will be restarted.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Question 1\n### Levenshtein Distance Algorithm"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a358f767-8c23-4024-bf9d-be899b5d82f6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from spellchecker import SpellChecker\nimport nltk\nnltk.download('punkt')\n\n# Initialize the spell checker\nspell = SpellChecker()\n\n# The sentence with typos to be fixed.\nsentence = \"this is asignmnt 2 and it is jsut great!\"\nfixed_sentence = \"\"\n\n# Loop through each word in the sentence and find the correct word using spellchecker.\nfor word in nltk.word_tokenize(sentence):\n    # Check if the word is misspelled and correct it if it is.\n    corrected_word = spell.correction(word)\n    # Add the corrected word to the fixed sentence.\n    fixed_sentence += corrected_word + \" \"\n\nprint(fixed_sentence.replace('2', '3'))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bb4a03f-eb09-4306-8fb9-2fea213bdf42","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nthis is assignment 3 and it is just great ! \n"]}],"execution_count":0},{"cell_type":"markdown","source":["Here there are two typos in the sentence:\n1. just\n- Replace the 's' with a 'u' and the 'u' with an 's'. (2 replacements).\n\n2. assignment\n- Insert characters 's' and and 'e' to fix the word typo. (2 insertions).\n\nThe number 2 was replaced with a 3 without the need for the algorithm."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9cff708c-b5b5-4c26-ae3e-2d6ba7abac07","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Question 2\n### Stack Overflow Similarity Analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb389617-ca6f-4b5d-9c66-fad567b8c12a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Define a function to load a CSV file into a PySpark DataFrame\ndef load(file):\n    # Use the read method of the SparkSession to read the CSV file\n    df = (spark.read\n        .option(\"multiline\", \"true\")  # Enable reading multiline data\n        .option(\"quote\", '\"')         # Set the quote character\n        .option(\"header\", \"true\")     # Treat the first row as the header row\n        .option(\"escape\", \"\\\\\")       # Set the escape character for backslashes\n        .option(\"escape\", '\"')        # Set the escape character for quotes\n        .csv(filepath)\n        )\n    \n    return df\n\n# Load the three CSV files using the load function\ndf_spark = load_csv('/FileStore/tables/SO_Spark.csv')\ndf_ml = load_csv('/FileStore/tables/SO_ML.csv')\ndf_security = load_csv('/FileStore/tables/SO_Security.csv')\n\n# Cast the Score column to integer type for each DataFrame\ndf_spark = df_spark.withColumn(\"Score\", df_spark[\"Score\"].cast(\"int\"))\ndf_ml = df_ml.withColumn(\"Score\", df_ml[\"Score\"].cast(\"int\"))\ndf_security = df_security.withColumn(\"Score\", df_security[\"Score\"].cast(\"int\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f4baf0a-c3d1-475a-9845-93f1d6502b9d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Define a UDF to merge two columns\n@udf\ndef merge_cols(a, b):\n    \"\"\"\n    UDF to merge two columns\n    \"\"\"\n    c = a + \" \" + b  \n    return c\n\n# Apply the UDF to create a new column \"TitleBody\" that merges \"Title\" and \"Body\" columns, and drop \"Body\" column\ndf_spark = df_spark.select(\"*\", merge_cols(\"Title\", \"Body\").alias(\"Title_and_Body\")).drop(\"Body\")\ndf_ml = df_ml.select(\"*\", merge_cols(\"Title\", \"Body\").alias(\"Title_and_Body\")).drop(\"Body\")\ndf_security = df_security.select(\"*\", merge_cols(\"Title\", \"Body\").alias(\"Title_and_Body\")).drop(\"Body\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67fe33e4-791f-4c35-9089-522293dd4a57","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 1. Preprocessing of Text"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fcdca8b6-a76f-49a3-81f5-24985a0624f6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Import the BeautifulSoup library to parse HTML\nfrom bs4 import BeautifulSoup\n\n# Define a UDF (user-defined function) to preprocess text data\n@udf\ndef preprocess_text(text):\n\n    # Parse the input text as HTML using BeautifulSoup\n    soup = BeautifulSoup(text)\n\n    # Find all hyperlinks in the HTML and remove them if present\n    urls = soup.find_all('a')\n    if len(urls) > 0:\n        soup.a.clear()\n\n    # Find all code blocks in the HTML and remove them if present\n    codes = soup.find_all('code')\n    if len(codes) > 0:\n        soup.code.clear()\n\n    # Find all pre-formatted text blocks in the HTML and remove them if present\n    pres = soup.find_all('pre')\n    if len(pres) > 0:\n        soup.pre.clear()\n\n    # Extract the remaining text from the cleaned HTML and convert it to lowercase\n    text = soup.get_text().lower()\n\n    # Return the preprocessed text\n    return text\n\n# Apply the preprocess_text UDF to the \"Title_and_Body\" column of each DataFrame,\n# creating a new column called \"Text\", and drop the original \"Title_and_Body\" column\ndf_spark = df_spark.select(\"*\", preprocess_text(\"Title_and_Body\").alias(\"Text\")).drop(\"Title_and_Body\")\ndf_ml = df_ml.select(\"*\", preprocess_text(\"Title_and_Body\").alias(\"Text\")).drop(\"Title_and_Body\")\ndf_security = df_security.select(\"*\", preprocess_text(\"Title_and_Body\").alias(\"Text\")).drop(\"Title_and_Body\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11606272-102b-4ff1-9510-c80670a6ba6d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### a) Tokenization and removing stopwords"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a31a6002-60cb-49c7-bb08-0f02a686357e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Import the necessary NLTK modules\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Define a UDF (user-defined function) to remove stopwords from text data\n@udf\ndef remove_all_stopwords(text):\n  \n    # Load the list of English stopwords from NLTK\n    stopwords = nltk.corpus.stopwords.words('english')\n\n    # Tokenize the input text into sentences using NLTK's sentence tokenizer\n    sentences = nltk.sent_tokenize(text)\n\n    # Initialize an empty list to store the filtered words\n    filt_words = []\n\n    # Loop over each sentence in the tokenized text\n    for sentence in sentences:\n        # Tokenize the sentence into words using NLTK's word tokenizer\n        words = nltk.word_tokenize(sentence)\n\n        # Loop over each word in the tokenized sentence\n        for word in words:\n            # Skip any words that are less than 3 characters long\n            if len(word) < 3:\n                continue\n\n            # Skip any words that are in the list of stopwords\n            if word in stopwords:\n                continue\n\n            # Add the word to the list of filtered words\n            filt_words.append(word)\n\n    # Return the filtered words as a list\n    return filt_words\n\n# Apply the remove_all_stopwords UDF to create a new column \"Filtered_Text\" that removes all stopwords in \"Text\" column, and drop \"Text\" column\ndf_spark = df_spark.select(\"*\", remove_all_stopwords(\"Text\").alias(\"Filtered_Text\")).drop(\"Text\")\ndf_ml = df_ml.select(\"*\", remove_all_stopwords(\"Text\").alias(\"Filtered_Text\")).drop(\"Text\")\ndf_security = df_security.select(\"*\", remove_all_stopwords(\"Text\").alias(\"Filtered_Text\")).drop(\"Text\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8cb3b435-e72d-44cc-b2dd-a01e62eb3b87","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### b) Removing noise"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de731124-fffe-40ec-88c2-a22af1ed3edf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Import the regular expression module\nimport re\n\n# Define a regular expression pattern to match punctuation characters\npunct_regex = re.compile('^[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+$')\n\n# Define a UDF (user-defined function) called remove_punct to remove punctuation from text data\n@udf\ndef remove_punct(text):\n    \n    # Initialize an empty list to store the cleaned text\n    cleaned_text = []\n\n    # Loop through each word in the input text\n    for word in text:\n        # Use the regular expression pattern to check if the word contains only punctuation characters\n        if bool(punct_regex.match(word)) == False:\n            # If the word does not contain only punctuation, add it to the cleaned text list\n            cleaned_text.append(word)\n\n    # Return the cleaned text list as the output of the UDF\n    return cleaned_text\n\n# Apply the remove_punct UDF to create a new column \"Text_no_punct\" that removes punctuation from \"Filtered_Text\" column, and drop \"Filtered_Text\" column\ndf_spark = df_spark.select(\"*\", remove_punct(\"Filtered_Text\").alias(\"Text_no_punct\")).drop(\"Filtered_Text\")\ndf_ml = df_ml.select(\"*\", remove_punct(\"Filtered_Text\").alias(\"Text_no_punct\")).drop(\"Filtered_Text\")\ndf_security = df_security.select(\"*\", remove_punct(\"Filtered_Text\").alias(\"Text_no_punct\")).drop(\"Filtered_Text\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b94b8f21-903f-48f4-ab34-969cffdee17b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### c) Stemming the text"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10a5a6b7-b844-409b-a4ac-f239a4678f97","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Import the Natural Language Toolkit (nltk) module\nimport nltk\n\n# Define a UDF (user-defined function) called stem_words to stem words in text data\n@udf\ndef stem_words(text):\n\n    # Create a Snowball stemmer for the English language\n    stemmer = nltk.stem.SnowballStemmer(language='english')\n\n    # Initialize an empty list to store the stemmed words\n    stemmed_text = []\n\n    # Loop over each word in the input text\n    for word in text:\n        # Use the Snowball stemmer to stem the word\n        stemmed_word = stemmer.stem(word)\n\n        # Add the stemmed word to the list of stemmed words\n        stemmed_text.append(stemmed_word)\n\n    # Return the list of stemmed words as the output of the UDF\n    return stemmed_text\n\n# Apply the stem_words UDF to create a new column \"Stemmed_Text\" that stems words in \"Text_Filtered\" column, and drop \"Text_Filtered\" column\ndf_spark = df_spark.select(\"*\", stem_words(\"Text_no_punct\").alias(\"Stemmed_Text\")).drop(\"Text_no_punct\")\ndf_ml = df_ml.select(\"*\", stem_words(\"Text_no_punct\").alias(\"Stemmed_Text\")).drop(\"Text_no_punct\")\ndf_security = df_security.select(\"*\", stem_words(\"Text_no_punct\").alias(\"Stemmed_Text\")).drop(\"Text_no_punct\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ab1e874-c73b-4173-98b7-f3549c0b6808","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Highest scored question with no accepted answer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2148e476-2cf0-4acd-8961-5aba27eb72ee","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Returns the highest-scoring unanswered question from a PySpark DataFrame of Stack Overflow questions.\ndef highest_unanswered_questions(df):\n    \n    # Filter the DataFrame to keep only unanswered questions\n    unanswered_df = df.filter(df[\"AcceptedAnswerId\"].isNull())\n    \n    # Sort the unanswered questions by score in descending order\n    unanswered_df = unanswered_df.sort(\"Score\", ascending=False)\n    \n    # Return the first row of the sorted DataFrame (i.e. the highest-scoring unanswered question)\n    return unanswered_df.first()\n\n# Call the highest_unanswered_questions function on each of the input PySpark DataFrames and store the resulting Rows in variables\nqs_spark = highest_unanswered_questions(df_spark)\nqs_ml = highest_unanswered_questions(df_ml)\nqs_security = highest_unanswered_questions(df_security)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86023f17-b313-417a-92f7-63fdca849023","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Defining the cosine similarity function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f64d4628-2f25-4990-82c6-1924c094274c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import math\nimport nltk\n\n# Define a UDF (user-defined function) to calculate the cosine similarity of top unanswered question (text2) to answered questions (text1)\n@udf\ndef cosine_similarity(text1):\n    \n    # Combine the unique words from both texts\n    d = list(set(text1 + text2))\n\n    # Initialize arrays to hold the presence/absence of words in each text\n    dt1 = []\n    dt2 = []\n\n    # Loop over each word in the combined set of words\n    for word in d:\n        # Check if the word is present in text t1\n        if word in text1:\n            dt1.append(1)\n        else:\n            dt1.append(0)\n\n        # Check if the word is present in text t2\n        if word in text2:\n            dt2.append(1)\n        else:\n            dt2.append(0)\n\n    # Calculate the numerator and denominator of the cosine similarity formula\n    numer = 0\n    sum_sq_dt1 = 0\n    sum_sq_dt2 = 0\n\n    for i in range(0, len(dt1)):\n        numer += dt1[i] * dt2[i]\n        sum_sq_dt1 += dt1[i] * dt1[i]\n        sum_sq_dt2 += dt2[i] * dt2[i]\n\n    denom = math.sqrt(sum_sq_dt1 * sum_sq_dt2)\n\n    # Calculate the cosine similarity score\n    if denom == 0:\n        return 0\n\n    return numer/denom\n\n\n# Define a function to return a PySpark DataFrame with a similarity column calculated using cosine similarity\ndef df_with_similarity(df):\n\n    # Filter the DataFrame to include only questions with an accepted answer, and calculate the cosine similarity score for each row\n    df = df.filter(df[\"AcceptedAnswerId\"].isNotNull()).select(\"*\", cosine_similarity(\"Stemmed_text\").alias(\"Similarity\")).sort(\"Similarity\", ascending=False)\n\n    # Return the DataFrame with the similarity column\n    return df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35b1a97a-5620-49f9-9167-ae5a29a8e79e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Calculating similarity scores for each question in all 3 files"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"83655e32-320e-489a-9cb2-93c40d5eea56","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Apply the df_with_similarity() function to each of the three DataFrames from earlier\n\ntext2 = qs_spark[\"Stemmed_Text\"].strip('][').split(', ')\ndf_spark_with_answers = df_with_similarity(df_spark)\n\ntext2 = qs_ml[\"Stemmed_Text\"].strip('][').split(', ')\ndf_ml_with_answers = df_with_similarity(df_ml)\n\ntext2 = qs_security[\"Stemmed_Text\"].strip('][').split(', ')\ndf_security_with_answers = df_with_similarity(df_security)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89f9f0b2-31f0-4fab-993b-0a9b8e4ce76f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Printing the recommended questions for the top unanswered question in each file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5aae215d-cb46-4f03-b34d-c48c411514e0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def print_recommendations(df, n):\n\n    # Get the first n row from the DF\n    num_top_questions = df.take(n)\n\n    # Loop over each row in the top n rows and print the Title, Id, and Similarity Score as a percentage with 2 decimal places\n    for i in range(n):\n        similarity_score = round(float(num_top_questions[i][\"Similarity\"]) * 100, 2)\n        print(\"  {}. [Id = {}, similarity score = {}%] {}\".format(i+1, num_top_questions[i][\"Id\"], similarity_score, num_top_questions[i][\"Title\"]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"05bb9153-5506-44b0-8345-310df07338ae","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Define a list of tuples, where each tuple contains a DataFrame and a string representing the name of the DataFrame\ndataframes = [(df_spark, \"SO_Spark\"), (df_ml, \"SO_ML\"), (df_security, \"SO_Security\")]\ndataframes_ans = [df_spark_with_answers, df_ml_with_answers, df_security_with_answers]\ni = 0\n\n# Loop over each tuple in the list\nfor df_tuple in dataframes:\n    # Get the highest scored question in the DataFrame\n    q = highest_unanswered_questions(df_tuple[0])\n    # Print the highest scored question\n    print(\"The highest scored question in {} without any accepted answer is:\\n  [Id = {}] {}\\n\".format(df_tuple[1], q[\"Id\"], q[\"Title\"]))\n    \n    # Print the top three recommendations with accepted answers using the print_recommendations function\n    print(\"Here are the top three recommendations with accepted answers:\")\n    print_recommendations(dataframes_ans[i], 3)\n    i+=1\n    print(\"\\n\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9a9d192-0665-466b-a4e0-718cb69857cc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["The highest scored question in SO_Spark without any accepted answer is:\n  [Id = 21138751] Spark java.lang.OutOfMemoryError: Java heap space\n\nHere are the top three recommendations with accepted answers:\n  1. [Id = 38223723, similarity score = 23.78%] Output file is getting generated on slave machine in apache spark\n  2. [Id = 44094070, similarity score = 23.57%] Why does standalone master schedule drivers on a worker?\n  3. [Id = 34324106, similarity score = 22.22%] Spark cluster: Standalone mode without HDFS\n\n\nThe highest scored question in SO_ML without any accepted answer is:\n  [Id = 4752626] Epoch vs Iteration when training neural networks\n\nHere are the top three recommendations with accepted answers:\n  1. [Id = 27959831, similarity score = 19.92%] How does Spark read 100K images effeciently?\n  2. [Id = 38536468, similarity score = 19.1%] Got OutOfMemory when run Spark MLlib kmeans\n  3. [Id = 54759966, similarity score = 17.77%] How to pass multiple Columns as features in a Logistic Regression Classifier in Spark?\n\n\nThe highest scored question in SO_Security without any accepted answer is:\n  [Id = 1592534] What is token-based authentication?\n\nHere are the top three recommendations with accepted answers:\n  1. [Id = 27899676, similarity score = 15.1%] Can user.name be spoofed\n  2. [Id = 61573031, similarity score = 14.71%] protecting stack memory of program\n  3. [Id = 67914209, similarity score = 14.21%] `Uncaught ReferenceError: SharedArrayBuffer is not defined` since Chrome 92\n\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ENSF 612 - A3","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2562867332109784,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":555786055384283}},"nbformat":4,"nbformat_minor":0}
